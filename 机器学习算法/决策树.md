# 决策树

- [ ] 决策树是一种基本的分类与回归方法，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型，预测时，对新的数据，利用训练好的决策树模型进行分类。

  注：也就是说，对于给定的训练数据，可能会得到多个决策树，我们的目的是在损失函数最小化的前提下，找到最优的决策树模型。

  ***决策树学习是由训练数据集估计条件概率模型。***

- [ ] 主要优点：模型具有可读性，分类速度快

- [ ] 决策树学习包含三个方面：
  1. 特征选择
  2. 决策树的生成
  3. 决策树的剪枝

### 特征选择

特征选择的目的：在于选取对训练数据具有分类能力的特征，从而可以提高决策树学习的效率。

（试想下，如果利用一个特征进行分类得到的结果，与随机分类得到的结果没差别，那么这个特征还有什么意义呢？因此，去掉这个特征对结果不会又太大的影响。）

决策树构造的关键在于选择最优划分属性（特征）。所谓划分属性就是在某个节点处按照某一特征属性划分构造出的分支，***越纯越好***。

#### 通常特征选择的准则，使用信息增益、信息增益率、基尼指数

1. 信息增益 (ID3)

   在介绍信息增益之前，我们首先了解一下信息熵(information entropy)。

   信息熵：是度量样本集合纯度的一种指标。不确定性越大，熵值越大，不确定性越小，熵值越小。（若样本集合只包含一种类别，则熵值最小为0；若样本集合分布式均匀分布，熵值最大）

   信息熵的定义：
   $$
   Ent(D)=-\sum^{|y|}_{k=1}p_{k}\log_{2}p_{k},\\
   其中，D表示样本集合，|y|表示样本集合中类别总数, \\
   Ent(D)的值越小，则D的纯度越高。
   $$
   假定，现在用离散属性a来对样本集D进行划分，a有V个可能的取值$\{a^{1},a^{2},...,a^{V}\}$，若使用a来对样本集D进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为$a^{v}$的样本，记为$D^{v}$。

   因此，属性a对样本集D进行划分所获得的信息增益为：
   $$
   Gain(D,a) = Ent(D) - \sum^{V}_{v=1}\frac{|D^{v}|}{|D|}Ent(D^{v}).
   $$
   注：信息增益越大，意味着使用属性a来进行划分获得的“纯度提升”越大。

   缺点：信息增益对包含可取值数目较多的属性有所偏好，为减少这种偏好带来的影响，引入了C4.5决策树算法，利用信息增益率划分属性。

2. 信息增益率 (C4.5)
   $$
   Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}, \\
   其中，\\
   IV(a) = -\sum^{V}_{v=1}\frac{|D^{v}|}{|D|}\log_{2}\frac{|D^{v}|}{|D|}.
   $$
   IV(a)称为属性a的 "固有值"。属性a的可能取值数目越多（即V越大），则 IV(a) 的值越大。

   注：信息增益率对可取值数目较少的属性有所偏好，因此，C4.5算法并不是直接选择信息增益率最大的候选划分属性，而是使用一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益率最高的。

3. 基尼指数

   CART决策树，使用基尼指数来选择划分属性。

   数据集D的纯度可以用基尼值来度量：
   $$
   \begin{eqnarray}
   Gini(D) &=& \sum^{|y|}_{k=1}\sum_{k^{'}\neq k}p_{k}p_{k^{'}} \\
           &=& 1 - \sum^{|y|}_{k=1}p^{2}_{k}.
   \end{eqnarray}
   $$
   直观来说，Gini(D) 反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据集D的纯度越高。

   属性a对于样本集D的基尼指数定义为：
   $$
   Gini\_index(D,a) = \sum^{V}_{v=1}\frac{|D^{v}|}{|D|}Dini(D^{v}).
   $$
   注：在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性。

### 剪枝处理

剪枝是决策树学习算法对付“过拟合”的主要手段。在决策树学习中，为了更好地学习训练样本，造成决策树分支过多，以致于把训练集自身分一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可以通过剪枝降低过拟合的风险。

1. 预剪枝

   指在决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；

2. 后剪枝

   是先生成一颗完整的决策树，然后从底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。